---
layout: post
title: Can We Use Machine Learning to Predict The Winning Team In League of Legends?
subtitle: Unit 2 Build Project And Blog Post
gh-repo: AidanMahoney73/DS-Unit-2-Applied-Modeling
gh-badge: [star, fork, follow]
tags: [test]
comments: true
---

League of Legends is a free to play online game where 2 teams face off against one another in hopes of beating the other in combat. First teams take turns picking and banning characters for the game. These characters have unique abilities and stats and players can choose items and spells as well.

To use machine learning on this problem I selected the winning team to be my target so that I could use the characters as my features. I found a DataFrame with nearly 50k rated games and sifted through it to train a Baseline model, Ridge Regression, and a Catboost model. In the data, there were columns that had scores and details regarding the played games that would aid in predicting the winner, so those columns were removed so as to ensure data leakage would be avoided

When training and validated to models I used the ROC AUC metric as it gives results in a percentage format (useful due to my Catboost model and Ridge Regression model using different number scales) and is compatible with linear models, unlike the sklearn accuracy metric. To make the baseline model I took the winners column and selected the team with the most wins to always predict as the winner. This yielded a ROC AUC score of .5 exactly. Upon training the linear model and getting the score of the validation set it yielded about .53525. This isn’t great but since its a binary classification problem the Catboost model should have an easier time making sense of the data. Unfortunately, the Catboost model only got about .5358 which is only just barely higher than the ridge regression model.

The Features are explaining very little about the data

![Unit 2 (1).png](/img/Unit 2 (1).png)

And the predictions the machine does make are very weak

Interestingly the model is better at prediction when team 1 will win than team 2. I believe maybe this is because of the Pick and Ban order of the game and how characters are decided in the first place

This model doesn’t seem useful for team predictions and the data given isn’t enough to grasp how the game will turn out. There are too many invisible factors such as player skill and team communication. Running the final test score on the Catboost model (my best model of the mix) I got a score of about .52681 which is a further decrease from the validation scores.











## The Basics of Chess
{: style="text-align: center;"}

“Chess is a finite, two-player, zero-sum, perfect-information game.” - [Sora](https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=no-game-no-life-2014&episode=s01e01)
{: style="text-align: center;"}

In Chess players take turns moving pieces on a board with the goal of capturing the enemy king to win the game. Luck plays no part in chess and the games are won by skill and foresight.

## My Dataset
{: style="text-align: center;"}

My dataset is from the user [Mitchell J on Kaggle.](https://www.kaggle.com/datasnaek/chess) This dataset was created using the Lichess API and collecting data from multiple users. The dataset contains 20058 games played by humans of varying strengths. The dataset has plenty of information for research including, but not limited to, both of the player’s ID, both player’s ELO score, the results of the match, the moves that were made, and the opening ECO code for the game.

## Exploring the Dataset
{: style="text-align: center;"}

In total there were 9107 wins by black, 950 draws, and 10001 wins by white. This means that Player 1 as white has an approximate 9.8% higher win ratio than Player 2 as black.

![all](/img/all.png)

Of the top 10 openings in the dataset, the highest win rate for white was the B00 ECO code, and the highest win ratio for black was B20.

The ECO code B00 is: pawn to e4

The ECO code B20 is: pawn to e4, pawn to c5

![top10eco](/img/top10eco.png)

This goes to show that Black can respond to whites moves and so long as each player had sufficient knowledge and skill the game goes back and forth. Remember Chess is a zero-sum game and anytime a player gains the other player loses an equal amount. There are 400 moves on the first full turn alone and not one of those 400 moves is capable of capturing a piece.

## Conclusion
{: style="text-align: center;"}

In the dataset, white seemingly does not have an advantage. The benefit felt by the players as white is very likely tied to the speed of being set up one move earlier. However, in getting setup first, black’s players have a better ability to respond to opposing moves given enough skill or pattern recognition.

[My Google Colab Notebook](https://colab.research.google.com/drive/158exZSw5ZTbXQSZ2tpjWIwWDIqkx3mmy)
{: style="text-align: center;"}
